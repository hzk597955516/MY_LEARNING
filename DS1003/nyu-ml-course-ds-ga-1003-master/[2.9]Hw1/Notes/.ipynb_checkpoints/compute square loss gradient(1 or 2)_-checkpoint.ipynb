{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 or 2?\n",
    "\n",
    "It is question in homework1(Machine Learning, DS-GA 1003, Spring 2019), 3.4 Batch Gradient Descent. The question is that if I change the scalar coefficient of the cost function from 1/2 to 1, the gradient decent will fail. This sounds impossible, but I just met it and now let's me give an description about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that:\n",
    "$$J(\\theta) = \\frac{1}{2m}(x \\theta - y)^{T}(x \\theta - y)$$\n",
    "$$J(\\theta) = \\frac{1}{m} (x \\theta - y)^{T} \\frac{\\partial (x \\theta - y)}{\\partial \\theta} = \\frac{1}{m} (x \\theta - y)^T x$$\n",
    "\n",
    "if we change the scalar coefficient of cost function to `1/m`, that's alright, and we get:\n",
    "$$J(\\theta) = \\frac{1}{m}(x \\theta - y)^{T}(x \\theta - y)$$\n",
    "$$J(\\theta) = \\frac{2}{m} (x \\theta - y)^{T} \\frac{\\partial (x \\theta - y)}{\\partial \\theta} = \\frac{2}{m} (x \\theta - y)^T x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code are as follows. \n",
    "\n",
    "Firstly, the data and `feature_normalization` function are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:02:27.297819Z",
     "start_time": "2019-07-29T03:02:17.438125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into Train and Test\n",
      "Scaling all to [0, 1]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature normalization\n",
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array\n",
    "                of size (num_instances, num_features)\n",
    "        test - test set, a 2D numpy array\n",
    "                of size (num_instances, num_features)\n",
    "\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized - test set after normalization\n",
    "\n",
    "    Example:\n",
    "    >>> data = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]])\n",
    "    >>> train_normalized, _ = feature_normalization(data, data)\n",
    "    >>> train_normalized\n",
    "    array([[0.  , 0.  ],\n",
    "           [0.25, 0.25],\n",
    "           [0.5 , 0.5 ],\n",
    "           [1.  , 1.  ]])\n",
    "    \"\"\"\n",
    "    train_normalized = \\\n",
    "        (train - train.min(axis=0)) / (train.max(axis=0) - train.min(axis=0))\n",
    "    test_normalized = \\\n",
    "        (test - train.min(axis=0)) / (train.max(axis=0) - train.min(axis=0))\n",
    "    return train_normalized, test_normalized\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv('data.csv', delimiter=',')\n",
    "X = df.values[:, :-1]\n",
    "y = df.values[:, -1]\n",
    "\n",
    "print('Split into Train and Test')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=100,\n",
    "                                                    random_state=10)\n",
    "\n",
    "print(\"Scaling all to [0, 1]\")\n",
    "X_train, X_test = feature_normalization(X_train, X_test)\n",
    "X_train = np.hstack((X_train, np.ones(\n",
    "    (X_train.shape[0], 1))))  # Add bias term\n",
    "X_test = np.hstack((X_test, np.ones(\n",
    "    (X_test.shape[0], 1))))  # Add bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `1/2m` works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:02:27.307705Z",
     "start_time": "2019-07-29T03:02:27.302243Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# The square loss function\n",
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the average square loss \n",
    "    for predicting y with X*theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array\n",
    "            of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array\n",
    "            of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the average square loss, scalar\n",
    "\n",
    "    Example:\n",
    "    >>> X = np.array([[1, 2], [2, 4], [3, 6]])\n",
    "    >>> theta = np.array([1, 1])\n",
    "    >>> y = np.array([3, 3, 9])\n",
    "    >>> compute_square_loss(X, y, theta)\n",
    "    3.0\n",
    "    \"\"\"\n",
    "    # Initialize the average square loss\n",
    "    num_instances = X.shape[0]\n",
    "    abs_loss = X @ theta - y\n",
    "    return (0.5 / num_instances) * (abs_loss.T @ abs_loss)\n",
    "\n",
    "# The gradient of the square loss function\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the average square loss\n",
    "    (as defined in compute_square_loss), at the point theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array\n",
    "            of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    num_instances = X.shape[0]\n",
    "    return 1 / num_instances * (X @ theta - y).T @ X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the gradient's coefficient is `1 / num_instances` because we use `1/2m` in cost function.\n",
    "\n",
    "And we can see this works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:02:27.859518Z",
     "start_time": "2019-07-29T03:02:27.310455Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the average square loss objective.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array\n",
    "            of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_step - number of steps to run\n",
    "        grad_check - a boolean value indicating whether checking\n",
    "                     the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector,\n",
    "                    2D numpy array of size (num_step+1, num_features)\n",
    "                    for instance, theta in step 0 should be theta_hist[0],\n",
    "                    theta in step (num_step) is theta_hist[-1]\n",
    "        loss_hist - the history of average square loss on the data,\n",
    "                    1D numpy array, (num_step+1)\n",
    "    \"\"\"\n",
    "    _, num_features = X.shape[0], X.shape[1]\n",
    "    # Initialize theta_hist\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))\n",
    "    # Initialize loss_hist\n",
    "    loss_hist = np.zeros(num_step + 1)\n",
    "    # Initialize theta\n",
    "    theta = np.zeros(num_features)\n",
    "    for step_i in range(num_step + 1):\n",
    "        theta_hist[step_i, ] = theta\n",
    "        loss_hist[step_i] = compute_square_loss(X, y, theta)\n",
    "        if grad_check:\n",
    "            assert grad_checker(X, y, theta)\n",
    "        theta -= compute_square_loss_gradient(X, y, theta) * alpha\n",
    "    return theta_hist, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:02:28.022368Z",
     "start_time": "2019-07-29T03:02:27.867086Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_hist, loss_hist = batch_grad_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:02:28.094484Z",
     "start_time": "2019-07-29T03:02:28.024913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.98075917, 3.91325976, 3.85188424, ..., 1.03900245, 1.03892598,\n",
       "       1.03884969])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `1/m` works bad\n",
    "\n",
    "here we hold everything unchanged except the `compute_square_loss_gradient` function, we just change the `1` in last line to `2`. After that, everything changed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:02:28.192554Z",
     "start_time": "2019-07-29T03:02:28.097152Z"
    }
   },
   "outputs": [],
   "source": [
    "# The gradient of the square loss function\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the average square loss\n",
    "    (as defined in compute_square_loss), at the point theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array\n",
    "            of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    num_instances = X.shape[0]\n",
    "    return 2 / num_instances * (X @ theta - y).T @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:02:28.342529Z",
     "start_time": "2019-07-29T03:02:28.194538Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shensir/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in matmul\n",
      "/home/shensir/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in matmul\n",
      "/home/shensir/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in matmul\n",
      "/home/shensir/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in matmul\n",
      "/home/shensir/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in matmul\n"
     ]
    }
   ],
   "source": [
    "theta_hist_2, loss_hist_2 = batch_grad_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:02:28.423908Z",
     "start_time": "2019-07-29T03:02:28.345912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.98075917, 3.98386419, 5.04193667, ...,        nan,        nan,\n",
       "              nan])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_hist_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:02:28.725212Z",
     "start_time": "2019-07-29T03:02:28.426345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb3302c96a0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the plot\n",
    "plt.plot(loss_hist_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I am very confusing, and I don't know where is the problem. Any help will be appreciated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
