#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Homework 6: Multiclass, Trees and Gradient Boosting
\end_layout

\begin_layout Author
Lin Pengyun
\end_layout

\begin_layout Section
Reformulation of Multiclass Hinge Loss
\end_layout

\begin_layout Subsection
Multiclasss setting review
\end_layout

\begin_layout Standard
There is no question in this part.
\end_layout

\begin_layout Subsection
Two version of multiclass hinge loss
\end_layout

\begin_layout Enumerate
We have: 
\begin_inset Formula 
\[
\max_{y\in\mathcal{Y}}f(y)=\max_{y\in\mathcal{Y}-\{y_{i}\}}\{\max[f(y_{i}),f(y)]\}
\]

\end_inset


\begin_inset Newline newline
\end_inset

If 
\begin_inset Formula $\Delta(y,y)=0$
\end_inset

, then 
\begin_inset Formula $\Delta(y_{i},y_{i})+h(x_{i},y_{i})-h(x_{i},y_{i})=0$
\end_inset

, we have: 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\max_{y\in\mathcal{Y}}[\Delta(y_{i},y)+h(x_{i},y)-h(x_{i},y_{i})] & = & \max_{y\in\mathcal{Y}-\{y_{i}\}}\{\max[0,\Delta(y_{i},y)+h(x_{i},y)-h(x_{i},y_{i})]\}\\
m_{i,y}(h) & = & h(x_{i},y_{i})-h(x_{i},y)
\end{eqnarray*}

\end_inset


\begin_inset Newline newline
\end_inset

So we have: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\max_{y\in\mathcal{Y}}[\Delta(y_{i},y)+h(x_{i},y)-h(x_{i},y_{i})=\max_{y\in\mathcal{Y}-\{y_{i}\}}\{\max[0,\Delta(y_{i},y)-m_{i,y}(h)]\}
\]

\end_inset


\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $\forall y\in\mathcal{Y},m_{i,y}(h)\ge\Delta(y_{i},y)$
\end_inset

, we have: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
l_{1}(h,(x_{i},y_{i}))=l_{2}(h,(x_{i},y_{i}))=0
\]

\end_inset


\begin_inset Newline newline
\end_inset

The classification creteria is 
\begin_inset Formula $f(x_{i})=\arg\max_{y\in\mathcal{Y}}h(x_{i},y)$
\end_inset

, 
\begin_inset Formula $m_{i,y}(h)=h(x_{i},y_{i})-h(x_{i},y)\ge0$
\end_inset

 and the equality holds only when 
\begin_inset Formula $y=y_{i}$
\end_inset

.
\begin_inset Newline newline
\end_inset

Hence, 
\begin_inset Formula $f(x_{i})=y_{i}$
\end_inset

 in the conditions decribed in this question.
\end_layout

\begin_layout Section
SGD for Multiclass Linear SVM
\end_layout

\begin_layout Enumerate
If for a set of functions 
\begin_inset Formula $\{f_{1}(x),...,f_{n}(x)\}$
\end_inset

, 
\begin_inset Formula $f_{i}(x)$
\end_inset

 is convex function, then: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\max_{i}f_{i}[\alpha x+(1-\alpha)y]\le\max_{i}\alpha f_{i}(x)+(1-\alpha)f_{i}(y)\le\alpha\max_{i}f_{i}(x)+(1-\alpha)\max_{i}f_{i}(y)
\]

\end_inset


\begin_inset Newline newline
\end_inset

So 
\begin_inset Formula $\max_{i}f_{i}(x)$
\end_inset

 is also convex.
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
J(w)=\lambda w^{T}w+\frac{1}{n}\sum_{i=1}^{n}\max_{y\in\mathcal{Y}}[\Delta(y_{i},y)+<w,\Psi(x_{i},y)-\Psi(x_{i},y_{i})>]
\]

\end_inset


\begin_inset Newline newline
\end_inset

The norm and inner product of 
\begin_inset Formula $w$
\end_inset

 are both convex, for the additivity of convex function and max invariance
 shown before, 
\begin_inset Formula $J(w)$
\end_inset

 is also convex
\end_layout

\begin_layout Enumerate
The subgradient of 
\begin_inset Formula $\lambda w^{T}w$
\end_inset

 w.r.t.
 
\begin_inset Formula $w$
\end_inset

 is 
\begin_inset Formula $2\lambda w$
\end_inset

.
\begin_inset Newline newline
\end_inset

Denote 
\begin_inset Formula $\hat{y_{i}}$
\end_inset

 as 
\begin_inset Formula $\mathop{\arg\max}_{y\in\mathcal{Y}}[\Delta(y_{i},y)+<w,\Psi(x_{i},y)-\Psi(x_{i},y_{i})>]$
\end_inset

, one of the subgradients of each 
\begin_inset Formula $\max$
\end_inset

 term is: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\Psi(x_{i},\hat{y_{i}})-\Psi(x_{i},y_{i})
\]

\end_inset


\begin_inset Newline newline
\end_inset

So one subgradient of 
\begin_inset Formula $J(w)$
\end_inset

 could be: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
2\lambda w+\frac{1}{n}\sum_{i=1}^{n}\Psi(x_{i},\hat{y}_{i})-\Psi(x_{i},y_{i})
\]

\end_inset


\end_layout

\begin_layout Enumerate
According to 2, the subgradient on 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

 could be: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
2\lambda w+\Psi(x_{i},\hat{y}_{i})-\Psi(x_{i},y_{i})
\]

\end_inset


\end_layout

\begin_layout Enumerate
The minibatch subgradient based on 
\begin_inset Formula $(x_{i},y_{i}),...,(x_{i+m},y_{i+m})$
\end_inset

 is: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
2\lambda w+\frac{1}{m}\sum_{k=i}^{i+m}\Psi(x_{i},\hat{y}_{i})-\Psi(x_{i},y_{i})
\]

\end_inset


\end_layout

\begin_layout Section
Hinge Loss is a Special Case of Generalized Hinge Loss
\end_layout

\begin_layout Standard
When the problem reduces to binary classification where the output space
 
\begin_inset Formula $\mathcal{Y}=\{-1,1\}$
\end_inset

, the loss function on 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

 is: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\max_{y\in\{-1,1\}}[\Delta(y_{i},y)+h(x_{i},y)-h(x_{i},y_{i})]
\]

\end_inset


\begin_inset Newline newline
\end_inset

 Denote the score function of binary SVM is 
\begin_inset Formula $g(x)$
\end_inset

, let: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{align*}
\Delta(-1,1) & =\Delta(1,-1)=1,\Delta(1,1)=\Delta(-1,-1)=0\\
h(x_{i},y_{i}) & =\frac{y_{i}}{2}g(x_{i})
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $y\in\{-1,1\}=\{-y_{i},y_{i}\}$
\end_inset

, the loss function will be: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\max[0,1-y_{i}g(x)],
\]

\end_inset


\begin_inset Newline newline
\end_inset

which is as same as the that of binary SVM.
\end_layout

\begin_layout Section
Multiclass classification–Implementation
\end_layout

\begin_layout Subsection
One-vs-All (or One-vs-Rest)
\end_layout

\begin_layout Enumerate
Code is in another repository.
 The result is shown below.
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename D:/Code_uc/Python_ML_Alg/figures/hw6/on_vs_all_svm.png
	width 10cm
	height 6.5cm

\end_inset


\end_layout

\begin_layout Subsection
Multiclass-SVM
\end_layout

\begin_layout Enumerate
Code is in another repository.
 The result is shown below.
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename D:/Code_uc/Python_ML_Alg/figures/hw6/multiclass_svm.png
	width 10cm
	height 6.5cm

\end_inset


\end_layout

\begin_layout Section
Audio Classification
\end_layout

\begin_layout Standard
This part is optional, now I just leave it unfinished.
\end_layout

\begin_layout Section
Decision Tree Implementation
\end_layout

\begin_layout Enumerate
The fitting result of classification decision tree is below: 
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename D:/Code_uc/Python_ML_Alg/figures/hw6/clf_tree.png
	width 10cm
	height 6.5cm

\end_inset


\begin_inset Newline newline
\end_inset

As the depth's growing, the fitting result is generally becoming better
 and better, but overfit appears in the last two pictures.
\end_layout

\begin_layout Enumerate
The fitting result of regression tree w.r.t.
 one-dimensional data is below: 
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename D:/Code_uc/Python_ML_Alg/figures/hw6/rgr_tree.png
	width 10cm
	height 6.5cm

\end_inset


\begin_inset Newline newline
\end_inset

Fitting result goes better while overfit to noise aggravates as the depth
 grows.
\end_layout

\begin_layout Section
Gradient Boosting Machine
\end_layout

\begin_layout Enumerate
If the prediction function is 
\begin_inset Formula $f(x)$
\end_inset

 and the loss function is given by: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
l(y,\hat{y})=\frac{1}{2}(y-\hat{y})^{2},\hat{y}=f(x)
\]

\end_inset


\begin_inset Newline newline
\end_inset

then the partial derivative on 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

 w.r.t.
 the 
\begin_inset Formula $(m-1)$
\end_inset

'th round prediction function 
\begin_inset Formula $f_{m-1}(x)$
\end_inset

 is: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\frac{\partial}{\partial f_{m-1}(x_{i})}l(y_{i},f_{m-1}(x_{i}))=f_{m-1}(x_{i})-y_{i},
\]

\end_inset

so the gradient of 
\begin_inset Formula $g_{m}$
\end_inset

 is: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
g_{m}=(f_{m-1}(x_{1})-y_{1},...f_{m-1}(x_{n})-y_{n})^{T}
\]

\end_inset

According to the gradient boosting algorithm, the function selected in 
\begin_inset Formula $m$
\end_inset

'th round should be: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
h_{m} & = & \mathop{\arg\min}_{h\in\mathcal{F}}\sum_{i=1}^{n}[-g_{m,i}-h(x_{i})]^{2}\\
 & = & \mathop{\arg\min}_{h\in\mathcal{F}}\sum_{i=1}^{n}[y_{i}-f_{m-1}(x_{i})-h(x_{i})]^{2}
\end{eqnarray*}

\end_inset


\begin_inset Newline newline
\end_inset

Therefore, in every round of iteration, the algorithm always finds the function
 which fits best the residual in the former round.
 
\end_layout

\begin_layout Enumerate
If the loss function is given by: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
l(y,f(x))=\ln(1+e^{-yf(x)}),
\]

\end_inset

the derivative w.r.t.
 
\begin_inset Formula $f_{m-1}(x)$
\end_inset

 on 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

 is: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\frac{\partial}{\partial f_{m-1}(x_{i})}l(y_{i},f_{m-1}(x_{i}))=-\dfrac{y_{i}}{1+e^{y_{i}f_{m-1}(x_{i})}}
\]

\end_inset

So we have: 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
h_{m}=\mathop{\mathop{\arg\min}_{h\in\mathcal{F}}}\sum_{i=1}^{n}[\dfrac{y_{i}}{1+e^{y_{i}f_{m-1}(x_{i})}}-h(x_{i})]^{2}
\]

\end_inset


\end_layout

\begin_layout Section
Gradient Boosting–Implementataion
\end_layout

\begin_layout Enumerate
In this question, an gradient boosting algorithm with 
\begin_inset Formula $l_{2}$
\end_inset

 loss and base prediciton function of decision tree is used to perform a
 classfication task on 2-D data and regression task on 1-D dataset.
 The result is below: 
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename D:/Code_uc/Python_ML_Alg/figures/hw6/GBT_clf.png
	width 10cm
	height 6.5cm

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename D:/Code_uc/Python_ML_Alg/figures/hw6/GBT_rgr.png
	width 10cm
	height 6.5cm

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename D:/Code_uc/Python_ML_Alg/figures/hw6/GBT_skl.png
	width 10cm
	height 6.5cm

\end_inset


\begin_inset Newline newline
\end_inset

The last diagram is the prediction result of gradient boosting algorithm
 in sklearn.
\end_layout

\end_body
\end_document
