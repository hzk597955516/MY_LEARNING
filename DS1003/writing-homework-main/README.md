# writing-homework  

## Description  

 This project contains writing homework of [**DS1003, NYU**](https://nyu-ds1003.github.io/spring2021/#resources). Up to now, I have finished the former four homework and all of them are hand-written. All these writing homework will be converted to latex
format in the near future.  
This project is still ongoing now.  

## Brief Introduction to Every Homework  

### Homework1  

(1) Mathematical fundamentals: Linear algebra and probability theory.  
(2) Matrix Calculus: Reformulate functions by matrix operation to get a compact expression and utilize vectorization.  
(3) Bayes Prediction Functions: Derive Bayes prediction functions for various loss functions.  
(4) Statistical Learning Theory: Excess Risk Decomposition, regularization, overfit and underfit.  

### Homework2  

(1) Coordinate Descent: Derive Shooting Algorithm for Lasso regression.  
(2) Properties of Lasso Regression: Biggest regularization parameter and distribution of weight parameter.  

### Homework3  

(1) Subgradient: A property of subgradient of max function.  
(2) Perceptron Algorithm: Derive of subgradient descent algorithm for Perceptron.  
(3) Derivation of Pegasos Algorithm: Utilize stochastic subgradient descent to optimize SVM.  

### Homework4  

(1) Kernel Method: Derive kernelized form of Ridge Regression and compute optimal parameters.  
(2) Kernelized Pegasos: Derive stochastic subgradient descent method for kernelized SVM.  
(3) Representer Theorem: Prove this.  
(4) Ivanov Regularization and Tikhonov Regularization: Prove the equivalence between them.  

### Homework5  

(1) Loss Function for Probabilistic Models: Derive bayesian optimal functions for four loss functions in the setting of binary classification.  
(2) Logistic Regression: Show the equivalence between ERM of logistic loss function and negative log-likelihood. Tackle numeric overflow of log-sum-exp.Prove the convexity of logistic loss function.  
(3) Bayesian Logistic Regression: Derive the posterior of baysian logistic function and  show the equivalence between bayesian logistic regression and regularized logistic regression.  
(4) Bayesian Linear Regression: Derive the expression of bayesian linear regression and show the equivalence between it and regularized ridge regression.  
(5) Coin Flipping: Estimate probability of head with maximum likelihood approach and bayesian approach with Beta distribution prior.  
  
### Homework 6  
  
(1) Multiclass Hinge Loss: Analyze generalized hinge loss and its special form.  
(2) SGD for Multiclass SVM: Show the objective function is convex and derive the subgradient descent algorithm for multiclass SVM.
(3) Multiclass SVM and Decision Tree: Analyze the result of programming homework.  
(4) Gradient Boosting: Derive the algorithm for L2-boosting and logistic loss boosting.  
**Still ongoing.**
